{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "# answer\n",
        "Information Gain (IG) measures how much “information” a feature gives about the target variable.\n",
        "It is the reduction in entropy (uncertainty) achieved by splitting a dataset based on a feature.\n",
        "Decision Trees use Information Gain to decide which feature to split on at each node — the feature with the highest IG is chosen.\n",
        "\n",
        "High IG → good split (reduces uncertainty)\n",
        "\n",
        "Low IG → poor split (less informative)"
      ],
      "metadata": {
        "id": "d_lqPDPxWBbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "# Hint: Directly compares the two main impurity measures, highlighting strengths,weaknesses, and appropriate use cases.\n",
        "# ANSWER\n",
        "#                       Gini Impurity\n",
        ". 1 −∑𝑝𝑖21−∑pi2  \n",
        ". Measures probability of misclassification\n",
        ". Faster computation\n",
        " #                        Entropy\n",
        "\n",
        "  . −∑𝑝ilog⁡2(𝑝𝑖)−∑pilog2(pi)\n",
        ". Measures level of information disorder\n",
        ". When information theory understanding is needed\n",
        "**Key Points:**\n",
        "Both measure impurity (lower is better).\n",
        "Gini is simpler and faster → used in CART (Classification and Regression Trees).\n",
        "Entropy is used in ID3 and C4.5 decision tree algorithms."
      ],
      "metadata": {
        "id": "LyDCsOXwWYfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3:What is Pre-Pruning in Decision Trees?\n",
        "# ANSWER\n",
        "Pre-pruning (also called early stopping) stops the growth of the Decision Tree before it becomes overly complex.\n",
        "It sets conditions to halt splitting if:\n",
        "A node’s information gain is below a threshold.\n",
        "The number of samples in a node is too small.\n",
        "The maximum depth of the tree is reached.\n",
        "✅ Purpose: Prevent overfitting and improve generalization."
      ],
      "metadata": {
        "id": "WmALzxMwXuEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "# ANSWER\n",
        "\n"
      ],
      "metadata": {
        "id": "4bbOvwsOYEVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Classifier using Gini Impurity\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Output\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIPTNtwkYOcp",
        "outputId": "58df21e0-b305-46c6-e96b-9753de881326"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.000\n",
            "sepal width (cm): 0.019\n",
            "petal length (cm): 0.893\n",
            "petal width (cm): 0.088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: What is a Support Vector Machine (SVM)?\n",
        "# ANSWER\n",
        "SVM is a supervised learning algorithm used for classification and regression.\n",
        "It finds the optimal hyperplane that maximizes the margin (distance between classes).\n",
        "The data points nearest to the hyperplane are called Support Vectors.\n",
        "\n",
        "✅ Advantages:\n",
        ">>Works well in high-dimensional spaces\n",
        ">>Robust to overfitting in most cases"
      ],
      "metadata": {
        "id": "2R9UP_sXYUd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: What is the Kernel Trick in SVM?\n",
        "# ANSWER\n",
        "The Kernel Trick allows SVM to perform classification on non-linearly separable data by transforming it into a higher-dimensional space without explicitly computing the transformation.\n",
        "# Common Kernels:\n",
        "**Linear:** Simple, when data is linearly separable\n",
        "**Polynomial:** For curved decision boundaries\n",
        "**RBF (Radial Basis Function):** Most popular for non-linear data"
      ],
      "metadata": {
        "id": "dPcY84q9Ykmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n"
      ],
      "metadata": {
        "id": "1GHYy196Y8JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", linear_acc)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz9AYrHfZCAq",
        "outputId": "090d69ab-2309-4b6b-f4b6-b7665354144e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "# ANSWER\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem:\n",
        "\n",
        "𝑃(𝐶∣𝑋)=𝑃(𝑋∣𝐶)⋅𝑃(𝐶)𝑃(𝑋)P(C∣X)=P(X)P(X∣C)⋅P(C)\n",
        "\t​\n",
        "  It assumes all features are independent given the class — this is the “naïve” assumption, which simplifies computation but is rarely true in reality.\n",
        "\n",
        "✅ Advantages:\n",
        ">>Fast and efficient for large datasets\n",
        ">>Works well for text classification and spam filtering"
      ],
      "metadata": {
        "id": "Gzn1qkJUZFdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "# ANSWER\n",
        "# 1. Gaussian Naïve Bayes (GNB)\n",
        "** Theoretical Basis: **\n",
        "Assumes that features follow a normal (Gaussian) distribution within each class.\n",
        "It calculates the probability of a feature value using the probability density function (PDF) of a Gaussian distribution.\n",
        "𝑃(𝑥𝑖∣𝐶𝑘)=12𝜋𝜎𝑘2𝑒−(𝑥𝑖−𝜇𝑘)22𝜎𝑘2P(xi∣Ck2πσk21e−2σk2(xi−μk)2\n",
        "\t​\n",
        "**Used For: **\n",
        "Continuous features (real numbers like height, weight, temperature, etc.).\n",
        "** Example:**\n",
        "Classifying patients based on blood pressure and age.\n",
        "**Advantages:**\n",
        "Works well when data is continuous and roughly follows a bell-curve distribution.\n",
        "**Limitations:**\n",
        "Not suitable for text or count data, which is discrete.\n",
        "\n",
        "# 2. Multinomial Naïve Bayes (MNB)\n",
        "\n",
        "** Theoretical Basis:**\n",
        "Assumes that features represent discrete counts or frequencies (how many times an event occurs).\n",
        "It models the data using a multinomial distribution — suitable for count vectors.\n",
        "\n",
        "𝑃(𝑋∣𝐶𝑘)=(∑𝑖𝑥𝑖)!∏𝑖𝑥𝑖!∏𝑖𝑃(𝑥𝑖∣𝐶𝑘)𝑥𝑖P(X∣Ck)=∏ixi!(∑ixi)!i∏P(xi∣Ck)x\n",
        "\t​\n",
        "** Used For:**\n",
        "Text classification, spam detection, document categorization, etc.\n",
        "(e.g., number of times a word appears in a document).\n",
        "** Example:**\n",
        "Classifying emails as spam based on word frequencies.\n",
        "** Advantages:**\n",
        "Performs well on document classification and text mining tasks.\n",
        "** Limitations:**\n",
        "Not appropriate for continuous data — requires integer (count) inputs.\n",
        "\n",
        "# 3. Bernoulli Naïve Bayes (BNB)\n",
        "\n",
        "** Theoretical Basis:**\n",
        "Assumes binary features — each feature can take only two values: 1 (present/true) or 0 (absent/false).\n",
        "Uses the Bernoulli distribution to model feature probabilities.\n",
        "\n",
        "𝑃(𝑥𝑖∣𝐶𝑘)=𝑃𝑖𝑥𝑖(1−𝑃𝑖)1−𝑥𝑖P(xi∣Ck​)=Pixi(1−Pi​)1−xi\n",
        "** Used For:**\n",
        "Binary/Boolean data, where features indicate presence or absence.\n",
        "**Example:**\n",
        "Whether a document contains a specific word (yes/no).\n",
        "** Advantages:**\n",
        "Works well for binary features and short text classification tasks.\n",
        "**Limitations:**\n",
        "Loses information about frequency — it only checks if a feature exists, not how many times."
      ],
      "metadata": {
        "id": "X5eZkbm-ZgH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Breast Cancer Dataset Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "WNOHRyklhI1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBhlIyXahS_D",
        "outputId": "7734e81c-6212-4a36-ca23-5dea364ab431"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}